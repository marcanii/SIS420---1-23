{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Aprendizaje\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este ejemplo se implementara el algoritmo de backpropagation, o propagación hacia atras para redes neuronales y se aplicara a la tarea del reconocimiento de digitos manuscritos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils\n",
    "\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal\n",
    "\n",
    "En el ejercicio anterior se implemento la propagacion hacia adelante para realizar predicciones en este ejercicio se implementara la propagacion hacia atraz para entrenar para aprender los parametros para la red neuronal.\n",
    "\n",
    "Se inicia el ejercicio cargando el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "#  los datos de entrenamiento se almacenan en los arreglos X, y\n",
    "data = loadmat('ex4data1.mat')\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# Se cambia el valor de 10 por 0 de y\n",
    "y[y == 10] = 0\n",
    "\n",
    "# Numero de ejemplos de entrenamiento\n",
    "m = y.size\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'displayData'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m rand_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(m, \u001b[39m100\u001b[39m, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m sel \u001b[39m=\u001b[39m X[rand_indices, :]\n\u001b[1;32m----> 5\u001b[0m utils\u001b[39m.\u001b[39;49mdisplayData(sel)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'displayData'"
     ]
    }
   ],
   "source": [
    "# Se seleccionan 100 datos para ser visualizados\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "utils.displayData(sel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Representación del modelo\n",
    "\n",
    "![](Figures/neural_network.png)\n",
    "\n",
    "La red neuronal tiene 3 capas: una capa de entrada, una capa oculta y una capa de salida. Recuerde que las entradas son valores de píxeles de digitos de imagenes. Dado que las imágenes tienen un tamaño de $20 \\times 20$, esto nos da 400 unidades de capa de entrada (sin contar la unidad de oscilación adicional que siempre genera +1). Los datos de entrenamiento se cargaron en las variables `X` y `y` anteriores.\n",
    "\n",
    "Se proporciona un conjunto de parámetros de red ($\\Theta^{(1)}, \\Theta^{(2)}$) ya entrenados. Estos se almacenan en `ex4weights.mat` y se cargarán en la siguiente celda de este cuaderno en `Theta1` y `Theta2`. Los parámetros tienen dimensiones que están dimensionadas para una red neuronal con 25 unidades en la segunda capa y 10 unidades de salida (correspondientes a las clases de 10 dígitos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Tue Oct 18 14:57:02 2011', '__version__': '1.0', '__globals__': [], 'Theta1': array([[-2.25623899e-02, -1.05624163e-08,  2.19414684e-09, ...,\n",
      "        -1.30529929e-05, -5.04175101e-06,  2.80464449e-09],\n",
      "       [-9.83811294e-02,  7.66168682e-09, -9.75873689e-09, ...,\n",
      "        -5.60134007e-05,  2.00940969e-07,  3.54422854e-09],\n",
      "       [ 1.16156052e-01, -8.77654466e-09,  8.16037764e-09, ...,\n",
      "        -1.20951657e-04, -2.33669661e-06, -7.50668099e-09],\n",
      "       ...,\n",
      "       [-1.83220638e-01, -8.89272060e-09, -9.81968100e-09, ...,\n",
      "         2.35311186e-05, -3.25484493e-06,  9.02499060e-09],\n",
      "       [-7.02096331e-01,  3.05178374e-10,  2.56061008e-09, ...,\n",
      "        -8.61759744e-04,  9.43449909e-05,  3.83761998e-09],\n",
      "       [-3.50933229e-01,  8.85876862e-09, -6.57515140e-10, ...,\n",
      "        -1.80365926e-06, -8.14464807e-06,  8.79454531e-09]]), 'Theta2': array([[-0.76100352, -1.21244498, -0.10187131, -2.36850085, -1.05778129,\n",
      "        -2.20823629,  0.56383834,  1.21105294,  2.21030997,  0.44456156,\n",
      "        -1.18244872,  1.04289112, -1.60558756,  1.30419943,  1.37175046,\n",
      "         1.74825095, -0.23365648, -1.52014483,  1.15324176,  0.10368082,\n",
      "        -0.37207719, -0.61530019, -0.1256836 , -2.27193038, -0.71836208,\n",
      "        -1.29690315],\n",
      "       [-0.61785176,  0.61559207, -1.26550639,  1.85745418, -0.91853319,\n",
      "        -0.05502589, -0.38589806,  1.29520853, -1.56843297, -0.97026419,\n",
      "        -2.18334895, -2.85033578, -2.07733086,  1.63163164,  0.3490229 ,\n",
      "         1.82789117, -2.44174379, -0.8563034 , -0.2982564 , -2.07947873,\n",
      "        -1.2933238 ,  0.89982032,  0.28306578,  2.31180525, -2.46444086,\n",
      "         1.45656548],\n",
      "       [-0.68934072, -1.94538151,  2.01360618, -3.12316188, -0.2361763 ,\n",
      "         1.38680947,  0.90982429, -1.54774416, -0.79830896, -0.65599834,\n",
      "         0.7353833 , -2.58593294,  0.47210839,  0.55349499,  2.51255453,\n",
      "        -2.4167454 , -1.63898627,  1.2027302 , -1.20245851, -1.83445959,\n",
      "        -1.88013027, -0.34056098,  0.23692483, -1.06137919,  1.02759232,\n",
      "        -0.47690832],\n",
      "       [-0.67832479,  0.46299226,  0.58492321, -0.1650184 ,  1.93264192,\n",
      "        -0.22965765, -1.84731492,  0.49011768,  1.07146054, -3.31905643,\n",
      "         1.54113507,  0.37371947, -0.86484681, -2.58273522,  0.97062447,\n",
      "        -0.51021867, -0.68427897, -1.64713607,  0.21153145, -0.27422442,\n",
      "         1.72599755,  1.32418658, -2.63984479, -0.08055871, -2.03510803,\n",
      "        -1.46123776],\n",
      "       [-0.59664339, -2.04481799,  2.05698407,  1.95100909,  0.17637699,\n",
      "        -2.16141218, -0.40394736,  1.80157532, -1.56278739, -0.25253004,\n",
      "         0.23586497,  0.71656699,  1.07689092, -0.35457279, -1.67743058,\n",
      "        -0.12939255, -0.67488849,  1.14066535,  1.32431237,  3.21158484,\n",
      "        -2.15888898, -2.60164082, -3.2226466 , -1.89612906, -0.87488068,\n",
      "         2.51038628],\n",
      "       [-0.87794907,  0.4344112 , -0.93161049,  0.18390778, -0.36078216,\n",
      "         0.61958137,  0.38624948, -2.65150343,  2.29710773, -2.08818098,\n",
      "        -1.86382323,  1.06057836,  0.77562146,  2.1346861 , -1.14973702,\n",
      "        -0.52081426,  0.99743429, -1.48309353, -2.3139424 ,  0.29517333,\n",
      "        -0.38704879, -2.20607697,  0.30702191, -1.17646114, -1.63462966,\n",
      "        -0.82467661],\n",
      "       [-0.52746527,  1.21564288, -1.50095981, -2.03195359, -1.52366734,\n",
      "        -2.43732079, -2.37570311, -1.39987277, -0.88735315, -0.63278873,\n",
      "         1.50450176, -1.580763  ,  0.58599217, -0.77540416,  0.94257331,\n",
      "         2.10919653,  0.54479132,  0.43773612, -1.28024228, -0.04360994,\n",
      "         1.4774997 , -1.13276949, -0.72846904,  0.04734716,  1.6574566 ,\n",
      "         1.68540944],\n",
      "       [-0.7490154 , -0.72249056, -3.15228173,  0.36577778,  0.19811362,\n",
      "        -0.73059946,  1.65263918, -2.300357  , -1.87468162,  0.98095387,\n",
      "        -1.58825159,  1.35434142,  2.17895331, -1.99239762, -2.00371362,\n",
      "        -0.388613  , -2.33992976, -2.91719062,  0.99398645, -2.70476768,\n",
      "        -1.27139772,  1.86091461, -1.20519404, -0.38014194,  0.7087181 ,\n",
      "        -2.11014003],\n",
      "       [-0.6665468 ,  0.53601845,  1.30307573, -1.03372714, -4.03084753,\n",
      "         0.58173469, -2.65717902,  0.80379994, -1.09241928,  2.49910058,\n",
      "         0.362008  ,  0.66195337, -0.92160534, -0.83123666, -2.00200952,\n",
      "        -2.94897501,  0.64564202, -1.10114694,  0.74510309,  0.58506717,\n",
      "        -1.99545251,  0.62591105,  1.80596103, -0.22309315, -1.40442136,\n",
      "        -2.1319153 ],\n",
      "       [-0.46089119, -1.43944954, -1.21809509,  0.71093011,  0.45216919,\n",
      "        -0.35953381,  0.62284954, -0.67005297, -0.7069138 ,  0.06311351,\n",
      "        -1.23199074, -1.74645233, -2.71960897, -2.21437178, -1.69307505,\n",
      "        -0.90927394,  0.87852311,  1.18664814, -1.87041262,  0.39796295,\n",
      "         1.72113872, -1.36934055,  0.8580668 , -0.24779579,  1.28009118,\n",
      "        -1.32752042]])}\n"
     ]
    }
   ],
   "source": [
    "# Configurando parametros necesario\n",
    "input_layer_size  = 400  # Entrada de imagenes de digitos de 20x20\n",
    "hidden_layer_size = 25   # 25 unidades ocultas\n",
    "num_labels = 10          # 10 etiquetas, de 0 a 9\n",
    "\n",
    "# carga los pesos en las variables Theta1 y Theta2\n",
    "weights = loadmat('ex4weights.mat')\n",
    "\n",
    "print(weights)\n",
    "\n",
    "# Theta1 tiene un tamaño de 25x401\n",
    "# Theta2 tiene un tamañó de 10x26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "# se intercambia la ultima columa con la primera de Theta2, por cuestiones de indices que utiliza MATLAB\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "# Desenrollar parámetros\n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  8, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = ([3,4,5])\n",
    "a*b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "### 1.3 Propagación hacia adelante y funcion de costo\n",
    "\n",
    "Ahora se implementa la funcion de costo y gradiente para la red neuronal `nnCostFunction`.\n",
    "\n",
    "La función de costo para la red neuronal (sin regularización) es:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "Donde $h_\\theta \\left( x^{(i)} \\right)$ se calcula como se muestra en la figura de la red neuronal anterior, y K = 10 es el número total de etiquetas posibles. Tenga en cuenta que $h_\\theta(x^{(i)})_k = a_k^{(3)}$ es la activación (valor de salida) de la unidad de salida $k^{th}$. Además, recuerde que mientras que las etiquetas originales (en la variable y) eran 0, 1, ..., 9, con el propósito de entrenar una red neuronal, necesitamos codificar las etiquetas como vectores que contienen solo valores 0 o 1, entonces:\n",
    "\n",
    "$$ y = \n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\\\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\cdots  \\quad \\text{or} \\qquad\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Por ejemplo, si $x^{(i)}$ es una imagen del dígito 5, entonces el $y^{(i)}$ correspondiente (que debe usar con la función de costo) debe ser un vector de 10 dimensiones con $y_5 = 1$, y los otros elementos iguales a 0.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "Nota de implementación: ** La matriz $X$ contiene los ejemplos en filas (es decir, X [i ,:] es el i-ésimo ejemplo de entrenamiento $x^{(i)}$, expresado como $n \\times 1$ vector.) Cuando complete el código en `nnCostFunction`, deberá agregar la columna de 1 a la matriz X. Los parámetros para cada unidad en la red neuronal se representan en Theta1 y Theta2 como una fila. Específicamente, la primera fila de Theta1 corresponde a la primera unidad oculta en la segunda capa. Puede utilizar un bucle for sobre los ejemplos para calcular el costo.\n",
    "    \n",
    "</div>\n",
    "<a id=\"nnCostFunction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de la función sigmoidea evaluada en z.\n",
    "    Esto debería funcionar independientemente de si z es una matriz o un vector. \n",
    "    En particular, si z es un vector o una matriz, debe devolver el gradiente para cada elemento.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the gradient of the sigmoid function evaluated at\n",
    "    each value of z (z can be a matrix, vector or scalar).\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    We have provided an implementation of the sigmoid function \n",
    "    in `utils.py` file accompanying this assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    g = utils.sigmoid(z) * (1 - utils.sigmoid(z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Implementa la función de costo de la red neuronal y el gradiente para una red neuronal de dos capas que realiza la clasificación.    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector. This needs to be converted back into the weight matrices Theta1\n",
    "        and Theta2.\n",
    "    \n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer. \n",
    "    \n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer.\n",
    "    \n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer. \n",
    "    \n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size).\n",
    "    \n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function at the current weight values.\n",
    "    \n",
    "    grad : array_like\n",
    "        An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "        neural network weights Theta1 and Theta2.\n",
    "    \n",
    "    \n",
    "    Note \n",
    "    ----\n",
    "    We have provided an implementation for the sigmoid function in the file \n",
    "    `utils.py` accompanying this assignment.\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    m = y.size\n",
    "         \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    \n",
    "    a2 = utils.sigmoid(a1.dot(Theta1.T))\n",
    "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n",
    "    \n",
    "    a3 = utils.sigmoid(a2.dot(Theta2.T))\n",
    "    \n",
    "    y_matrix = y.reshape(-1)\n",
    "    y_matrix = np.eye(num_labels)[y_matrix]\n",
    "    \n",
    "    temp1 = Theta1\n",
    "    temp2 = Theta2\n",
    "    \n",
    "    # Agregar el termino de regularización\n",
    "    \n",
    "    reg_term = (lambda_ / (2 * m)) * (np.sum(np.square(temp1[:, 1:])) + np.sum(np.square(temp2[:, 1:])))\n",
    "    \n",
    "    J = (-1 / m) * np.sum((np.log(a3) * y_matrix) + np.log(1 - a3) * (1 - y_matrix)) + reg_term\n",
    "    \n",
    "    # Backpropogation\n",
    "    \n",
    "    delta_3 = a3 - y_matrix\n",
    "    delta_2 = delta_3.dot(Theta2)[:, 1:] * sigmoidGradient(a1.dot(Theta1.T))\n",
    "\n",
    "    Delta1 = delta_2.T.dot(a1)\n",
    "    Delta2 = delta_3.T.dot(a2)\n",
    "    \n",
    "    # Agregar regularización al gradiente\n",
    "\n",
    "    Theta1_grad = (1 / m) * Delta1\n",
    "    Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n",
    "    \n",
    "    Theta2_grad = (1 / m) * Delta2\n",
    "    Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n",
    "    \n",
    "    # ===================== Alterntate solutions =====================\n",
    "    # my_final_matrix = np.zeros(a3.shape)\n",
    "    # for c in np.arange(num_labels):\n",
    "    #    my_final_matrix[:, c] = (np.log(a3[:, c]) * (y == c)) + (np.log(1 - a3[:, c]) * (1 - (y == c)))\n",
    "    #J = (-1 / m) * np.sum(my_final_matrix)\n",
    "    # ================================================================\n",
    "    \n",
    "    # ================================================================\n",
    "    # Unroll gradients\n",
    "    # grad = np.concatenate([Theta1_grad.ravel(order=order), Theta2_grad.ravel(order=order)])\n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez terminado, se llama a `nnCostFunction` usando el conjunto de parámetros cargados para` Theta1` y `Theta2`. Debería ver que el costo es de aproximadamente 0.287629."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'sigmoid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m lambda_ \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m J, _ \u001b[39m=\u001b[39m nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCosto en parametros (cargado de ex4weights): \u001b[39m\u001b[39m%.6f\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m J)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEl costo debe esta cercano a               : 0.287629\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 64\u001b[0m, in \u001b[0;36mnnCostFunction\u001b[1;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\u001b[0m\n\u001b[0;32m     60\u001b[0m Theta2_grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(Theta2\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     62\u001b[0m a1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([np\u001b[39m.\u001b[39mones((m, \u001b[39m1\u001b[39m)), X], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m a2 \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49msigmoid(a1\u001b[39m.\u001b[39mdot(Theta1\u001b[39m.\u001b[39mT))\n\u001b[0;32m     65\u001b[0m a2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([np\u001b[39m.\u001b[39mones((a2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)), a2], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m a3 \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39msigmoid(a2\u001b[39m.\u001b[39mdot(Theta2\u001b[39m.\u001b[39mT))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'sigmoid'"
     ]
    }
   ],
   "source": [
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\n",
    "print('Costo en parametros (cargado de ex4weights): %.6f ' % J)\n",
    "print('El costo debe esta cercano a               : 0.287629')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 1.4 Funcion de costo regularizada\n",
    "\n",
    "La funcion de costo para una red neuronal con regularizacion esta dado por:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n",
    "\n",
    "La red neuronal solo tendrá 3 capas: una capa de entrada, una capa oculta y una capa de salida. Sin embargo, el código debería funcionar para cualquier cantidad de unidades de entrada, unidades ocultas y unidades de salida. Mientras se han enumerado explícitamente los índices anteriores para $\\ Theta^{(1)}$ y $\\Theta^{(2)}$ para mayor claridad, tenga en cuenta que su código debería funcionar en general con $\\Theta^{(1)}$ y $\\Theta^{(2)} $ de cualquier tamaño. Hay que tomar en cuenta que no se debe regularizar los términos que corresponden al sesgo. Para las matrices `Theta1` y` Theta2`, esto corresponde a la primera columna de cada matriz. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminado, la siguiente celda llamará a `nnCostFunction` usando el conjunto cargado de parámetros para `Theta1` y `Theta2`, y $\\lambda = 1$. El costo deberia ser aproximadamente 0.383770."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costo en parametros (cargado de ex4weights): 0.383770\n",
      "El costo debe esta cercano a               : 0.383770\n"
     ]
    }
   ],
   "source": [
    "# Weight regularization parameter (we set this to 1 here).\n",
    "lambda_ = 1\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                      num_labels, X, y, lambda_)\n",
    "\n",
    "print('Costo en parametros (cargado de ex4weights): %.6f' % J)\n",
    "print('El costo debe esta cercano a               : 0.383770')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Backpropagation (Propagación hacia atraz)\n",
    "\n",
    "En esta parte del ejercicio, implementará el algoritmo de retropropagación para calcular el gradiente de la función de costo de la red neuronal. Se deberá actualizar la función `nnCostFunction` para que devuelva un valor apropiado para `grad`. Una vez que haya calculado el gradiente, podrá entrenar la red neuronal minimizando la función de costo $J(\\theta)$ utilizando un optimizador avanzado como `scipy`'s` optimize.minimize`.\n",
    "\n",
    "Primero se implementará el algoritmo de retropropagación para calcular los gradientes de los parámetros de la red neuronal (no regularizada). Una vez que se haya verificado que su cálculo de gradiente para el caso no regularizado es correcto, se implementará el gradiente para la red neuronal regularizada."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "### 2.1 Gradiente de la sigmoide\n",
    "\n",
    "El gradiente para la función sigmoidea puede ser calculada por:\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz} g(z) = g(z)\\left(1-g(z)\\right) $$\n",
    "\n",
    "donde\n",
    "\n",
    "$$ \\text{sigmoid}(z) = g(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda se llama a `sigmoidGradient` en un vector dado `z`. Intente probar algunos valores llamando a `sigmoidGradient (z)`. Para valores grandes (tanto positivos como negativos) de z, el gradiente debe ser cercano a 0. Cuando $z = 0$, el gradiente debe ser exactamente 0.25. El código también debería funcionar con vectores y matrices. Para una matriz, su función debe realizar la función de gradiente sigmoide en cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradiente sigmoide evaluada con [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "z = np.array([-1, -0.5, 0, 0.5, 1])\n",
    "g = sigmoidGradient(z)\n",
    "print('Gradiente sigmoide evaluada con [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "print(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Random Initialization\n",
    "\n",
    "Al entrenar redes neuronales, es importante inicializar aleatoriamente los parámetros para romper la simetría. Una estrategia eficaz para la inicialización aleatoria es seleccionar aleatoriamente valores para $\\Theta^{(l)}$ uniformemente en el rango $[-\\epsilon_{init}, \\epsilon_ {init}]$. Debe usar $\\epsilon_{init}=0.12$. Este rango de valores asegura que los parámetros se mantengan pequeños y hace que el aprendizaje sea más eficiente.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "Una estrategia eficaz para elegir $\\epsilon_{init}$ es basarlo en la cantidad de unidades en la red. Una buena elección de $\\epsilon_{init}$ es $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$ donde $L_{in} = s_l$ y $L_{out} = s_{l+1}$ son el número de unidades en las capas adyacentes a $\\Theta^{l}$.\n",
    "</div>\n",
    "\n",
    "```python\n",
    "# Randomly initialize the weights to small values\n",
    "W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "```\n",
    "Tenga en cuenta ue se da a la función un argumento para $\\epsilon$ con el valor predeterminado `epsilon_init = 0.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int\n",
    "        Number of incomming connections.\n",
    "    \n",
    "    L_out : int\n",
    "        Number of outgoing connections. \n",
    "    \n",
    "    epsilon_init : float, optional\n",
    "        Range of values which the weight can take from a uniform \n",
    "        distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like\n",
    "        The weight initialiatized to random values.  Note that W should\n",
    "        be set to a matrix of size(L_out, 1 + L_in) as\n",
    "        the first column of W handles the \"bias\" terms.\"\"\"\n",
    "        \n",
    "\n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicialización de parámetros de redes neuronales...\n"
     ]
    }
   ],
   "source": [
    "print('Inicialización de parámetros de redes neuronales...')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Desenrrollr parametros\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 2.4 Backpropagation\n",
    "\n",
    "![](Figures/ex4-backpropagation.png)\n",
    "\n",
    "Ahora, se implementará el algoritmo de retropropagación. Recuerde que la intuición detrás del algoritmo de retropropagación es la siguiente. Dado un ejemplo de entrenamiento $(x^{(t)}, y^{(t)})$, primero ejecutaremos un \"pase hacia adelante\" para calcular todas las activaciones en toda la red, incluido el valor de salida de la hipótesis $h_\\theta(x)$. Luego, para cada nodo $j$ en la capa $l$, se busca calcular un \"término de error\" $\\delta_j^{(l)}$ que mide cuánto ese nodo fue \"responsable\" de cualquier error en la salida.\n",
    "\n",
    "Para un nodo de salida, se puede medir directamente la diferencia entre la activación de la red y el valor objetivo real, y usar eso para definir $\\delta_j^{(3)}$ (ya que la capa 3 es la capa de salida). Para las unidades ocultas, calculará $\\delta_j^{(l)}$ basándose en un promedio ponderado de los términos de error de los nodos en la capa $(l+1)$. En detalle, aquí está el algoritmo de retropropagación (también representado en la figura anterior). Debe implementar los pasos 1 a 4 en un ciclo que procese un ejemplo a la vez. Concretamente, debe implementar un bucle for `for t in range(m)` y colocar los pasos 1-4 a continuación dentro del bucle for, con la iteración $t^{th}$ realizando el cálculo en el $t^{th}$ ejemplo de entrenamiento $(x^{(t)}, y^{(t)})$. El paso 5 dividirá los gradientes acumulados por $m$ para obtener los gradientes para la función de costo de la red neuronal.\n",
    "\n",
    "1. Establecer los valores de la capa de entrada $(a^{(1)})$ con los $t^{th }$ ejemplos de entrenamiento $x^{(t)}$. Ejecutar un paso de propagacion hacia adelante, calculando la activación $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ para las capas 2 y 3. Recordad que se debe añadir el termino de oscilacion `+1` termino que asegura que los vectores de de las capas de activacion $a^{(1)}$ y $a^{(2)}$ tambien incluyan una unidad de oscilacion (bias). En `numpy`, si a_1 es un columna de matriz, agregar una columna se locgra con `a_1 = np.concatenate([np.ones((m, 1)), a_1], axis=1)`.\n",
    "\n",
    "1. Por cada unidad de salida $k$ en la capa 3 (La capa de salida), establece \n",
    "$$\\delta_k^{(3)} = \\left(a_k^{(3)} - y_k \\right)$$\n",
    "donde $y_k \\in \\{0, 1\\}$ indica si el ejemplo de entrenamiento actual pertenece a la clase $k$ $(y_k = 1)$, o si pertenece a una clase diferente $(y_k = 0)$. Puede encontrar útiles las matrices lógicas para esta tarea (explicada en el ejercicio de programación anterior).\n",
    "\n",
    "1. Para la capa oculta se establece $l = 2$, \n",
    "$$ \\delta^{(2)} = \\left( \\Theta^{(2)} \\right)^T \\delta^{(3)} * g'\\left(z^{(2)} \\right)$$\n",
    "Tomar en cuenta que el simbolo  $*$ realiza multiplicaciones de elementos en `numpy`.\n",
    "\n",
    "1. Acumule el gradiente de este ejemplo usando la siguiente fórmula. Tenga en cuenta que debe omitir o eliminar $\\delta_0^{(2)}$. En `numpy`, eliminando $\\delta_0^{(2)}$ corresponde a `delta_2 = delta_2[1:]`.\n",
    "\n",
    "1. Obtenga el gradiente (no regularizado) para la función de costo de la red neuronal dividiendo los gradientes acumulados por $\\frac{1}{m}$:\n",
    "$$ \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$$\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Consejo de Python/Numpy **: debe implementar el algoritmo backpropagation solo después de haber completado con éxito las funciones de forwardpropagation y costo. Al implementar el algoritmo de retropropagación, a menudo es útil utilizar la función `shape` para imprimir las formas de las variables con las que está trabajando si se encuentra con errores de discrepancia de dimensión.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de que haya implementado el algoritmo de retropropagación, se procede a ejecutar la verificación del gradiente en la implementación. La verificación de gradiente permitirá aumentar la confianza en que el código calcula los gradientes correctamente.\n",
    "\n",
    "### 2.4  Comprobación del gradiente \n",
    "\n",
    "En la red neuronal, se está minimizando la función de costo $J(\\Theta)$. Para realizar una verificación de gradiente en sus parámetros, se puede imaginar \"desenrollar\" los parámetros $\\Theta^{(1)}$, $\\Theta^{(2)}$ en un vector largo $\\theta$. Al hacerlo, se puede pensar que la función de costo es $J(\\Theta)$ y usar el siguiente procedimiento de verificación de gradiente.\n",
    "\n",
    "Suponga que tiene una función $f_i(\\theta)$ que supuestamente calcula $\\frac{\\partial}{\\partial\\theta_i} J(\\theta)$; le gustaría comprobar si $f_i$ está generando valores derivados correctos.\n",
    "\n",
    "$$\n",
    "\\text{Let } \\theta^{(i+)} = \\theta + \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad \\theta^{(i-)} = \\theta - \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Entonces, $\\theta^{(i+)}$ es lo mismo que $\\theta$, excepto que su elemento $i^{th}$ se ha incrementado en $\\epsilon$. De manera similar, $\\theta^{(i−)}$ el vector correspondiente con el elemento $i^{th}$ disminuido en $\\epsilon$. Ahora se puede verificar numericamente la exactitud de $f_i(\\theta)$ comprobando, para cada $i$, que:\n",
    "\n",
    "$$ f_i\\left( \\theta \\right) \\approx \\frac{J\\left( \\theta^{(i+)}\\right) - J\\left( \\theta^{(i-)} \\right)}{2\\epsilon} $$\n",
    "\n",
    "El grado en el que estos dos valores deben aproximarse entre sí dependerá de los detalles de $J$. Pero suponiendo que $\\epsilon = 10^{-4}$, normalmente encontrará que los lados izquierdo y derecho de lo anterior coincidirán con al menos 4 dígitos significativos (y a menudo muchos más).\n",
    "\n",
    "Se tiene implementada la funcion de calculo numerico del gradiente en `computeNumericalGradient` (dentro del archivo `utils.py`). \n",
    "\n",
    "En la siguiente celda, ejecutaremos la función provista `checkNNGradients` que creará una pequeña red neuronal y un conjunto de datos que se utilizará para verificar sus gradientes. Si su implementación de retropropagación es correcta, debería ver una diferencia relativa menor que 1e-9.\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "** Consejo práctico **: al realizar la verificación de gradientes, es mucho más eficiente utilizar una pequeña red neuronal con un número relativamente pequeño de unidades de entrada y unidades ocultas, por lo que tiene un número relativamente pequeño\n",
    "de parámetros. Cada dimensión de $\\theta$ requiere dos evaluaciones de la función de costo y esto puede resultar costoso. En la función `checkNNGradients`, nuestro código crea un pequeño modelo aleatorio y un conjunto de datos que se usa con `computeNumericalGradient` para verificar el gradiente. Además, una vez que esté seguro de que sus cálculos de gradiente son correctos, debe desactivar la verificación de gradiente antes de ejecutar su algoritmo de aprendizaje.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "** Sugerencia práctica: ** La verificación del gradiente funciona para cualquier función en la que esté calculando el costo y el gradiente. Concretamente, puede usar la misma función `computeNumericalGradient` para verificar si sus implementaciones de gradiente para los otros ejercicios también son correctas (por ejemplo, la función de costo de regresión logística).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.27825235e-03 -9.27825236e-03]\n",
      " [-3.04978709e-06 -3.04978914e-06]\n",
      " [-1.75060084e-04 -1.75060082e-04]\n",
      " [-9.62660640e-05 -9.62660620e-05]\n",
      " [ 8.89911959e-03  8.89911960e-03]\n",
      " [ 1.42869450e-05  1.42869443e-05]\n",
      " [ 2.33146358e-04  2.33146357e-04]\n",
      " [ 1.17982666e-04  1.17982666e-04]\n",
      " [-8.36010761e-03 -8.36010762e-03]\n",
      " [-2.59383093e-05 -2.59383100e-05]\n",
      " [-2.87468729e-04 -2.87468729e-04]\n",
      " [-1.37149709e-04 -1.37149706e-04]\n",
      " [ 7.62813550e-03  7.62813551e-03]\n",
      " [ 3.69883257e-05  3.69883234e-05]\n",
      " [ 3.35320351e-04  3.35320347e-04]\n",
      " [ 1.53247082e-04  1.53247082e-04]\n",
      " [-6.74798369e-03 -6.74798370e-03]\n",
      " [-4.68759764e-05 -4.68759769e-05]\n",
      " [-3.76215583e-04 -3.76215587e-04]\n",
      " [-1.66560294e-04 -1.66560294e-04]\n",
      " [ 3.14544970e-01  3.14544970e-01]\n",
      " [ 1.64090819e-01  1.64090819e-01]\n",
      " [ 1.64567932e-01  1.64567932e-01]\n",
      " [ 1.58339334e-01  1.58339334e-01]\n",
      " [ 1.51127527e-01  1.51127527e-01]\n",
      " [ 1.49568335e-01  1.49568335e-01]\n",
      " [ 1.11056588e-01  1.11056588e-01]\n",
      " [ 5.75736494e-02  5.75736493e-02]\n",
      " [ 5.77867378e-02  5.77867378e-02]\n",
      " [ 5.59235296e-02  5.59235296e-02]\n",
      " [ 5.36967009e-02  5.36967009e-02]\n",
      " [ 5.31542052e-02  5.31542052e-02]\n",
      " [ 9.74006970e-02  9.74006970e-02]\n",
      " [ 5.04575855e-02  5.04575855e-02]\n",
      " [ 5.07530173e-02  5.07530173e-02]\n",
      " [ 4.91620841e-02  4.91620841e-02]\n",
      " [ 4.71456249e-02  4.71456249e-02]\n",
      " [ 4.65597186e-02  4.65597186e-02]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.41486e-11\n"
     ]
    }
   ],
   "source": [
    "utils.checkNNGradients(nnCostFunction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 2.5 Regularized Neural Network\n",
    "\n",
    "After you have successfully implemented the backpropagation algorithm, you will add regularization to the gradient. To account for regularization, it turns out that you can add this as an additional term *after* computing the gradients using backpropagation.\n",
    "\n",
    "Specifically, after you have computed $\\Delta_{ij}^{(l)}$ using backpropagation, you should add regularization using\n",
    "\n",
    "$$ \\begin{align} \n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\frac{\\lambda}{m} \\Theta_{ij}^{(l)} & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that you should *not* be regularizing the first column of $\\Theta^{(l)}$ which is used for the bias term. Furthermore, in the parameters $\\Theta_{ij}^{(l)}$, $i$ is indexed starting from 1, and $j$ is indexed starting from 0. Thus, \n",
    "\n",
    "$$\n",
    "\\Theta^{(l)} = \\begin{bmatrix}\n",
    "\\Theta_{1,0}^{(i)} & \\Theta_{1,1}^{(l)} & \\cdots \\\\\n",
    "\\Theta_{2,0}^{(i)} & \\Theta_{2,1}^{(l)} & \\cdots \\\\\n",
    "\\vdots &  ~ & \\ddots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "[Now modify your code that computes grad in `nnCostFunction` to account for regularization.](#nnCostFunction)\n",
    "\n",
    "After you are done, the following cell runs gradient checking on your implementation. If your code is correct, you should expect to see a relative difference that is less than 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.27825235e-03 -9.27825236e-03]\n",
      " [-1.67679797e-02 -1.67679797e-02]\n",
      " [-6.01744725e-02 -6.01744725e-02]\n",
      " [-1.73704651e-02 -1.73704651e-02]\n",
      " [ 8.89911959e-03  8.89911960e-03]\n",
      " [ 3.94334829e-02  3.94334829e-02]\n",
      " [-3.19612287e-02 -3.19612287e-02]\n",
      " [-5.75658668e-02 -5.75658668e-02]\n",
      " [-8.36010761e-03 -8.36010762e-03]\n",
      " [ 5.93355565e-02  5.93355565e-02]\n",
      " [ 2.49225535e-02  2.49225535e-02]\n",
      " [-4.51963845e-02 -4.51963845e-02]\n",
      " [ 7.62813550e-03  7.62813551e-03]\n",
      " [ 2.47640974e-02  2.47640974e-02]\n",
      " [ 5.97717617e-02  5.97717617e-02]\n",
      " [ 9.14587966e-03  9.14587966e-03]\n",
      " [-6.74798369e-03 -6.74798370e-03]\n",
      " [-3.26881426e-02 -3.26881426e-02]\n",
      " [ 3.86410548e-02  3.86410548e-02]\n",
      " [ 5.46101547e-02  5.46101547e-02]\n",
      " [ 3.14544970e-01  3.14544970e-01]\n",
      " [ 1.18682669e-01  1.18682669e-01]\n",
      " [ 2.03987128e-01  2.03987128e-01]\n",
      " [ 1.25698067e-01  1.25698067e-01]\n",
      " [ 1.76337550e-01  1.76337550e-01]\n",
      " [ 1.32294136e-01  1.32294136e-01]\n",
      " [ 1.11056588e-01  1.11056588e-01]\n",
      " [ 3.81928689e-05  3.81928696e-05]\n",
      " [ 1.17148233e-01  1.17148233e-01]\n",
      " [-4.07588279e-03 -4.07588279e-03]\n",
      " [ 1.13133142e-01  1.13133142e-01]\n",
      " [-4.52964427e-03 -4.52964427e-03]\n",
      " [ 9.74006970e-02  9.74006970e-02]\n",
      " [ 3.36926556e-02  3.36926556e-02]\n",
      " [ 7.54801264e-02  7.54801264e-02]\n",
      " [ 1.69677090e-02  1.69677090e-02]\n",
      " [ 8.61628953e-02  8.61628953e-02]\n",
      " [ 1.50048382e-03  1.50048382e-03]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.30858e-11\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 3.000000): 0.576051 \n",
      "(for lambda = 3, this value should be about 0.576051)\n"
     ]
    }
   ],
   "source": [
    "#  Check gradients by running checkNNGradients\n",
    "lambda_ = 3\n",
    "utils.checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _  = nnCostFunction(nn_params, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))\n",
    "print('(for lambda = 3, this value should be about 0.576051)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Learning parameters using `scipy.optimize.minimize`\n",
    "\n",
    "After you have successfully implemented the neural network cost function\n",
    "and gradient computation, the next step we will use `scipy`'s minimization to learn a good set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 100}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training completes, we will proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct. If your implementation is correct, you should see a reported\n",
    "training accuracy of about 95.3% (this may vary by about 1% due to the random initialization). It is possible to get higher training accuracies by training the neural network for more iterations. We encourage you to try\n",
    "training the neural network for more iterations (e.g., set `maxiter` to 400) and also vary the regularization parameter $\\lambda$. With the right learning settings, it is possible to get the neural network to perfectly fit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 97.080000\n"
     ]
    }
   ],
   "source": [
    "pred = utils.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred == y) * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visualizing the Hidden Layer\n",
    "\n",
    "One way to understand what your neural network is learning is to visualize what the representations captured by the hidden units. Informally, given a particular hidden unit, one way to visualize what it computes is to find an input $x$ that will cause it to activate (that is, to have an activation value \n",
    "($a_i^{(l)}$) close to 1). For the neural network you trained, notice that the $i^{th}$ row of $\\Theta^{(1)}$ is a 401-dimensional vector that represents the parameter for the $i^{th}$ hidden unit. If we discard the bias term, we get a 400 dimensional vector that represents the weights from each input pixel to the hidden unit.\n",
    "\n",
    "Thus, one way to visualize the “representation” captured by the hidden unit is to reshape this 400 dimensional vector into a 20 × 20 image and display it (It turns out that this is equivalent to finding the input that gives the highest activation for the hidden unit, given a “norm” constraint on the input (i.e., $||x||_2 \\le 1$)). \n",
    "\n",
    "The next cell does this by using the `displayData` function and it will show you an image with 25 units,\n",
    "each corresponding to one hidden unit in the network. In your trained network, you should find that the hidden units corresponds roughly to detectors that look for strokes and other patterns in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218fa209b70>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.displayData(Theta1[:, 1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
